{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1. Language Models, the Chat Format and Tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Install required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U python-dotenv\n",
    "\n",
    "#!pip install -U openai\n",
    "\n",
    "#!pip install -U tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Load the API key and relevant Python libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. A helper function to conveniently access OpenAI's LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Large Language Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 How Large Language Models work\n",
    "\n",
    "You're probably familiar with the **text generation process** where you can **give a prompt**,  \n",
    "\"I love eating\", and **ask an LLM to fill** in what the things are **likely completions**  \n",
    "given this prompt. And it may say, \"Bagels with cream cheese, or my mother's meatloaf, or out  \n",
    "with friends\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Large Language Models](./Images/llm_intro.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how did the model learn to do this? The main tool used to train an LLM  \n",
    "is actually **supervised learning**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Supervised Learning\n",
    "\n",
    " In supervised learning, a computer learns an **input-output** or **X or Y mapping**  \n",
    " using **labeled training data**. So for example, if you're using supervised learning  \n",
    " to learn to classify the sentiment of restaurant reviews, you might collect a training  \n",
    " set like this, where a review like, \"The pastrami sandwich is great!\", is labeled as a  \n",
    " positive  sentiment review, and so on. And \"Service was slow, the food was so-so.\",  \n",
    " it was negative, and \"The earl grey tea was fantastic.\", has a positive label. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for **supervised learning** is typically\n",
    " \n",
    " - get labeled data\n",
    " -  train AI model on data\n",
    " - deploy the model and call it\n",
    " \n",
    "If we give it a new restaurant review like **best pizza I've ever had**. You  \n",
    "hopefully output that as a **positive sentiment**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Supervised Learning](./Images/supervised_learning.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 LLM using Supervised Learning\n",
    "\n",
    "It turns out that **supervised learning is a core building block** for **training Large Language Models**.  \n",
    "Specifically, a **Large Language Model can be built by using supervised learning to repeatedly predict the**  \n",
    "**next word**. Let's say that in your training sets of a lot of text data, you have to sentence, \"My favorite food  \n",
    "is a bagel with cream cheese and lox.\". Then this sentence is turned into a sequence of training examples,  \n",
    "where given a sentence fragment, \"My favorite food is a\", if you want to predict the next word in this case was  \n",
    "\"bagel\", or given the sentence fragment or sentence prefix, \"My favorite food is a bagel\", the next word in this  \n",
    "case would be \"with\", and so on. And given a large training set of hundreds of billions or sometimes even more  \n",
    "words, you can then create a massive training set where you can start off with part of a sentence or part of a  \n",
    "piece of text and repeatedly ask the language model to learn to predict what is the next word. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LLM form Supervised Learning](./Images/llm_from_supervised_learning.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Types of LLM\n",
    "\n",
    "There are broadly two major types of Large Language Models.\n",
    "\n",
    " - Base LLM\n",
    " \n",
    "   The base LLM repeatedly predicts the next word based on text training data. And so if I give it a  \n",
    "   prompt, \"Once upon a time there was a unicorn\", then it may, by repeatedly predicting one word at a  \n",
    "   time, come up with a completion that tells a story about a unicorn living in a magical forest with  \n",
    "   all her unicorn friends. A downside of this is that if you were to prompt it with \"What is the capital  \n",
    "   of France?\", quite possible that on the internet there might be a list of quiz questions about France.  \n",
    "   So it may complete this with \"What is France's largest city, what is France's population?\", and so on. \n",
    "   \n",
    " - Instruction Tuned LLM\n",
    "\n",
    "   Instruction Tuned LLM instead tries to follow instructions and will hopefully say, \"The capital of France  \n",
    "   is Paris.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Types of LLMs](./Images/types_of_llms.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 How to get Instruction Tuned LLM from Base LLm\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Instruction Tuned LLM from Base LLm](./Images/base_llm_to_instruction_tuned_llsm.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Some example sentence completition using LLm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)\n",
    "# The capital of France is Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"Take the letters in lollipop \\\n",
    "and reverse them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)\n",
    "# pillipol"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the LLM failed to follow the instruction in the second case, which appears  \n",
    "to be a simple task. It turns out that there's one more important detail for how a Large  \n",
    "Language Model works, which is it doesn't actually repeatedly predict the next word, it  \n",
    "instead repeatedly predicts the **next token**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Tokens\n",
    "\n",
    "An LLM takes a sequence of characters, like \"Learning new things is fun!\", and group the  \n",
    "characters together to form tokens that comprise **commonly occurring sequences of characters**.  \n",
    "if you were to give it the word **lollipop**, the tokenizer actually breaks this down into three  \n",
    "tokens, **\"l\"**, **\"oll\"** and **\"ipop\"**. And because LLM isn't seeing the individual letters,  \n",
    "is instead seeing these three tokens, it's more difficult for it to correctly print out these  \n",
    "letters in reverse order.\n",
    "\n",
    "If you pass it lollipop with dashes in between the letters, like **L-O-L-L-I-P-O-P** it tokenizes  \n",
    "each of these characters into an individual token, making it easier for it to see the individual  \n",
    "letters and print them out in reverse order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an LLM\n",
    "\n",
    " - Input is often called **context**\n",
    "\n",
    " - Output is often called **target** or **completion**\n",
    "\n",
    " - The goal is to predict the target given the context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tokens](./Images/tokens.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"Take the letters in l-o-l-l-i-p-o-p \\\n",
    "and reverse them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)\n",
    "# p-o-p-i-l-l-o-l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. System, User and Assistant Messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **system message** specifies the **overall tone** of what you want the **Large Language Model**  \n",
    "or **the assistant**to do, and the **user message** is a **specific instruction** that you wanted to carry  \n",
    "out given this higher level behavior that was specified in the system message. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![System, User and Assistant Messages](./Images/system_user_and_assistant.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. An example completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)\n",
    "\n",
    "# Oh, the happy carrot, so bright and cheery,\n",
    "# With a vibrant orange hue, never weary.\n",
    "# In the garden it grew, bathed in sun's gleam,\n",
    "# Bringing joy and laughter, like in a dream.\n",
    "\n",
    "# Its leafy green tops danced in the breeze,\n",
    "# While the carrot below stood tall with ease.\n",
    "# It had a big smile, so silly and grand,\n",
    "# Bringing happiness to all the land.\n",
    "\n",
    "# From the earth it was pulled, with a merry sound,\n",
    "# Into a salad, its purpose was found.\n",
    "# Oh, the happy carrot, so full of delight,\n",
    "# Spreading joy with every bite.\n",
    "\n",
    "# So remember, my friend, in your daily fare,\n",
    "# A happy carrot's love is always there.\n",
    "# Bring a smile to your face, let laughter ensue,\n",
    "# With a happy carrot, your day will be true."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. An example completion with length restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)\n",
    "\n",
    "# Once upon a time, there was a cheerful carrot named Charlie who grew up alongside a field of\n",
    "# vibrant sunflowers, spreading joy wherever he went."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. An example completion with a given tone and length restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who \\\n",
    "responds in the style of Dr Seuss. \\\n",
    "All your responses must be one sentence long.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "] \n",
    "response = get_completion_from_messages(messages, \n",
    "                                        temperature =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)\n",
    "# Once there was a happy carrot named Larry, who lived in a garden full of joy and was always merry."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. A helper function which returns token count along with completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages, \n",
    "                                   model=\"gpt-3.5-turbo\", \n",
    "                                   temperature=0, \n",
    "                                   max_tokens=500):\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message[\"content\"]\n",
    "    \n",
    "    token_dict = {\n",
    "        'prompt_tokens':response['usage']['prompt_tokens'],\n",
    "        'completion_tokens':response['usage']['completion_tokens'],\n",
    "         'total_tokens':response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who responds\\\n",
    " in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem \\ \n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)\n",
    "\n",
    "# Oh, the happy carrot, so bright and orange,\n",
    "# Grown in the garden, a joyful forage.\n",
    "# With a smile so wide, from top to bottom,\n",
    "# It brings happiness, oh how it blossoms!\n",
    "\n",
    "# In the soil it grew, with love and care,\n",
    "# Nourished by sunshine, fresh air to share.\n",
    "# Its leaves so green, reaching up so high,\n",
    "# A happy carrot, oh my, oh my!\n",
    "\n",
    "# With a crunch and a munch, it's oh so tasty,\n",
    "# Filled with vitamins, oh so hasty.\n",
    "# A happy carrot, a delight to eat,\n",
    "# Bringing joy and health, oh what a treat!\n",
    "\n",
    "# So let's celebrate this veggie so grand,\n",
    "# With a happy carrot in each hand.\n",
    "# For in its presence, we surely find,\n",
    "# A taste of happiness, one of a kind!\n",
    "\n",
    "# print(token_dict)\n",
    "# {'prompt_tokens': 37, 'completion_tokens': 164, 'total_tokens': 201}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatGPT_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
